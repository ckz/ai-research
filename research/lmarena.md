# LMArena (formerly Chatbot Arena)

## Company Overview
| Attribute | Details |
|-----------|---------|
| **Company Name** | LMArena |
| **Origin** | UC Berkeley LMSYS (Large Model Systems Organization) |
| **Founded** | 2023 (research project), 2025 (commercial) |
| **Headquarters** | USA |
| **Category** | AI Evaluation / Benchmarking |
| **One-Sentence Description** | AI model evaluation platform at $1.7B valuation with 5M+ monthly users, $30M ARR in 4 months, standard for LLM benchmarking |

## Funding
| Attribute | Details |
|-----------|---------|
| **Total Funding** | $250M |
| **Seed Round** | $100M (May 2025) |
| **Series A** | $150M (January 2026) |
| **Valuation** | $1.7B |
| **Key Investors** | Felicis (Series A lead), UC Investments, Andreessen Horowitz, The House Fund, LDVP, Kleiner Perkins, Lightspeed Venture Partners, Laude Ventures |

## Funding History

### Series A (January 2026)
- Amount: $150M
- Lead: Felicis, UC Investments
- Valuation: $1.7B (nearly 3x from seed)
- Time Since Product Launch: 4 months

### Seed Round (May 2025)
- Amount: $100M
- Lead: Andreessen Horowitz, UC Investments
- Valuation: $600M

## Origin Story
- Started as UC Berkeley academic side project
- LMSYS (Large Model Systems Organization)
- "Chatbot Arena" original name
- Evolved into commercial company

## Key Metrics
- **Monthly Users**: 5M+ across 150 countries
- **Monthly Conversations**: 60M+
- **Commercial ARR**: $30M (December 2025)
- **Time to Revenue**: 4 months after launch

## Products & Technology

### Core Platform: Chatbot Arena
- Anonymous pairwise LLM comparisons
- Crowd-sourced voting
- Users compare two anonymous models
- Vote on better response
- Models revealed after vote

### AI Evaluations (Commercial - September 2025)
- Enterprise model evaluation service
- Developers hire for evaluations
- Community-based testing

## Industry Impact

### Models Tested
- OpenAI GPT-4o, o1
- Google DeepMind Gemini
- Anthropic Claude
- DeepSeek (tested R1 months before Western attention)
- Many others

### Usage by AI Labs
- Major companies use rankings for promotion
- Preview releases tested on platform
- Industry standard benchmark

## Competitive Position
Competing with:
- Hugging Face evaluations
- Stanford HELM
- EleutherAI evaluations
- Internal company benchmarks

### Differentiation
- Crowd-sourced (millions of votes)
- Anonymous testing = unbiased
- Industry-standard status
- Commercial enterprise service
- UC Berkeley academic credibility

## Strategic Notes
- 4 months to $30M ARR = exceptional
- $600M to $1.7B in 7 months = 3x
- DeepSeek used platform before breakout
- UC Berkeley origin = academic credibility
- 5M monthly users = massive scale
- 60M conversations = enormous dataset
- Industry-standard status = network effects
- AI labs as customers = strategic positioning

## Status
Active - Standard for AI model evaluation

## Sources
- TechCrunch, SiliconANGLE, The Information, PR Newswire, Wikipedia, Contrary Research
